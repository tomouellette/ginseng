{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"Get started","text":"<p><code>ginseng</code> is a python package for accurate, and somewhat efficient, cell type annotation using <code>jax</code> and <code>zarr</code>. It provides a variety of classes and functions in a simple-to-use interface for on-disk dataset creation, iterators for model training or data processing, and a lightweight neural attention model for cell type annotation.</p>","path":["Get started"],"tags":[]},{"location":"#installation","level":2,"title":"Installation","text":"<p><code>ginseng</code> can be installed using <code>pip</code></p> <pre><code>pip install ginseng\n</code></pre> <p>Or, added to your project using <code>uv</code></p> <pre><code>uv add ginseng\n</code></pre>","path":["Get started"],"tags":[]},{"location":"#license","level":2,"title":"License","text":"<p><code>ginseng</code> is licensed under the MIT License (see LICENSE).</p>","path":["Get started"],"tags":[]},{"location":"api/","level":1,"title":"API Reference","text":"","path":["API Reference"],"tags":[]},{"location":"api/#table-of-contents","level":2,"title":"Table of Contents","text":"<ul> <li>data/dataset.py<ul> <li>Class: GinsengDataset<ul> <li>Method: _load_metadata</li> <li>Method: create</li> <li>Method: make_split</li> <li>Method: stream</li> </ul> </li> </ul> </li> <li>data/io.py<ul> <li>Function: read_10x_mtx</li> <li>Function: read_10x_h5</li> <li>Function: read_adata</li> <li>Function: _load_pytree</li> <li>Function: save_model</li> <li>Function: _save_pytree</li> <li>Function: load_model</li> </ul> </li> <li>model/nn.py<ul> <li>Class: GinsengClassifier<ul> <li>Method: _get_key</li> <li>Method: predict</li> <li>Method: loss</li> <li>Method: evaluate</li> </ul> </li> <li>Function: nn_xavier_uniform</li> <li>Function: nn_init_linear</li> <li>Function: nn_linear</li> <li>Function: nn_dropout</li> <li>Function: nn_normalize</li> <li>Function: nn_annotate_init</li> <li>Function: nn_annotate</li> <li>Function: nn_annotate_loss</li> <li>Function: nn_annotate_evaluate</li> </ul> </li> <li>model/predict.py<ul> <li>Function: _get_gene_indices</li> <li>Function: annotate_iter</li> <li>Function: classify</li> </ul> </li> <li>model/state.py<ul> <li>Class: GinsengClassifierState</li> <li>Function: classifier_from_state</li> <li>Function: state_from_classifier_trainer</li> </ul> </li> <li>train/augment.py<ul> <li>Function: augment_mask</li> <li>Function: augment_background</li> <li>Function: augment_dropgene</li> <li>Function: augment</li> </ul> </li> <li>train/logger.py<ul> <li>Class: GinsengLogger<ul> <li>Method: update</li> <li>Method: report</li> </ul> </li> </ul> </li> <li>train/opt.py<ul> <li>Class: AdamState</li> <li>Function: opt_init_adam</li> <li>Function: opt_adam_update</li> </ul> </li> <li>train/trainer.py<ul> <li>Class: GinsengClassifierTrainerSettings</li> <li>Class: GinsengClassifierTrainer<ul> <li>Method: _train_step</li> <li>Method: fit</li> <li>Method: _run_epoch</li> <li>Method: _validate</li> </ul> </li> </ul> </li> <li>utils/hvg.py<ul> <li>Function: select_hvgs</li> </ul> </li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#file-datadatasetpy","level":2,"title":"File: <code>data/dataset.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#class-ginsengdataset","level":3,"title":"Class <code>GinsengDataset</code>","text":"<p>An on-disk dataset for training single-cell classifiers.</p> <p>Attributes:</p> <ul> <li>path (<code>Path</code>): Path to the zarr dataset on disk.</li> <li>root (<code>zarr.Group</code>): The root zarr group object.</li> <li>n_cells (<code>int</code>): Total number of cells (observations) in the dataset.</li> <li>n_genes (<code>int</code>): Total number of genes (variables) in the dataset.</li> <li>label_names (<code>list of str</code>): Human-readable names for the integer labels.</li> <li>gene_names (<code>list of str</code>): Names of the genes stored in the dataset.</li> <li>labels (<code>np.ndarray</code>): Integer labels for every cell in the dataset.</li> <li>groups (<code>np.ndarray or None</code>): Categorical group indices (e.g., batch or donor) if provided.</li> <li>train_idx (<code>np.ndarray or None</code>): Indices of cells assigned to the training split.</li> <li>test_idx (<code>np.ndarray or None</code>): Indices of cells assigned to the test split.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#method-_load_metadata","level":4,"title":"Method <code>_load_metadata</code>","text":"<p>Load metadata and small arrays into memory.</p>","path":["API Reference"],"tags":[]},{"location":"api/#method-create","level":4,"title":"Method <code>create</code>","text":"<p>Create a GinsengDataset from an AnnData object or file path.</p> <p>Parameters:</p> <ul> <li>path (<code>str | Path</code>): Output path where the zarr dataset directory will be created.</li> <li>adata (<code>str | Path | ad.AnnData</code>): Input data. Can be an AnnData object, a local path to a (.h5ad, .h5, or 10x directory), or a URL to a supported file format.</li> <li>label_key (<code>str</code>): The column name in <code>adata.obs</code> containing the target labels (e.g., cell type).</li> <li>layer (<code>str, optional</code>): The key in <code>adata.layers</code> to use for expression counts. If None, uses <code>adata.X</code> (default : None).</li> <li>genes (<code>str | list of str | np.ndarray, optional</code>): Gene selection/filtering logic. - If a string: Assumes it is a column in <code>adata.var</code> containing a boolean mask (e.g., \"highly_variable\"). - If a list or array: A specific set of gene names to keep. This will also reorder the output to match the provided list. - If None: Keeps all genes (default : None).</li> <li>group_key (<code>str, optional</code>): The column name in <code>adata.obs</code> containing grouping metadata, such as \"batch\" or \"donor\" (default : None).</li> <li>chunk_size (<code>int</code>): Number of rows (cells) per zarr chunk for the expression matrix. Larger chunks improve compression but require more RAM during streaming (default : 4096).</li> <li>overwrite (<code>bool</code>): Whether to delete the existing directory at <code>path</code> if it exists (default : True).</li> </ul> <p>Returns:</p> <ul> <li><code>GinsengDataset</code>: An initialized instance of the dataset pointing to the new zarr store.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#method-make_split","level":4,"title":"Method <code>make_split</code>","text":"<p>Create train/test splits and store indices on disk.</p> <p>Parameters:</p> <ul> <li>fraction (<code>float</code>): The proportion of data (or groups) to include in the test split. Must be in the range [0.0, 1.0) (default : 0.1).</li> <li>stratify_group (<code>bool</code>): If True, splits by groups (e.g., donors) rather than individual cells. Requires group_key to have been provided during creation (default : False).</li> <li>seed (<code>int</code>): Random seed for reproducibility (default : 123).</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#method-stream","level":4,"title":"Method <code>stream</code>","text":"<p>Stream mini-batches of (expression, label) from the zarr store.</p> <p>Parameters:</p> <ul> <li>batch_size (<code>int</code>): Number of cells to yield in each batch.</li> <li>split (<code>Literal[\"train\", \"test\", \"all\"]</code>): Which data split to stream from (default : \"train\").</li> <li>balance_labels (<code>bool</code>): If True, downsamples the training split to match the frequency of the least common class (default : False).</li> <li>shuffle (<code>bool</code>): Whether to shuffle the indices before streaming (default : True).  Yields ------</li> <li>X (<code>np.ndarray</code>): Expression matrix batch of shape (batch_size, n_genes).</li> <li>y (<code>np.ndarray</code>): Integer labels batch of shape (batch_size,).</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#file-dataiopy","level":2,"title":"File: <code>data/io.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#function-read_10x_mtx","level":3,"title":"Function <code>read_10x_mtx</code>","text":"<p>Read 10x Genomics mtx format into an AnnData object.</p> <p>Parameters:</p> <ul> <li>path (<code>str</code>): Path to directory containing matrix.mtx, barcodes.tsv, and genes.tsv/features.tsv.</li> <li>var_names (<code>str</code>): Select 'gene_symbols' or 'gene_ids' as index for var.</li> <li>make_unique (<code>bool</code>): If True, make var_names unique.</li> </ul> <p>Returns:</p> <ul> <li><code>AnnData</code>: </li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-read_10x_h5","level":3,"title":"Function <code>read_10x_h5</code>","text":"<p>Read 10x Genomics h5 format into an AnnData object.</p> <p>Parameters:</p> <ul> <li>path (<code>str</code>): Path to 10x Genomics .h5 file.</li> <li>genome (<code>str, optional</code>): Genome to extract (if file contains multiple genomes). If None, will use the first genome available.</li> <li>var_names (<code>str</code>): Select 'gene_symbols' or 'gene_ids' as index for var.</li> <li>make_unique (<code>bool</code>): If True, make var_names unique.</li> </ul> <p>Returns:</p> <ul> <li><code>AnnData</code>: </li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-read_adata","level":3,"title":"Function <code>read_adata</code>","text":"<p>Read an AnnData object from various supported file formats.</p> <p>Parameters:</p> <ul> <li>path (<code>str | Path</code>): Path to the input count data stored in 10x .h5 format, AnnData .h5ad format, or in a 10x matrix market format folder.</li> <li>backed (<code>bool</code>): If True and the input is an <code>.h5ad</code> file, open the file in backed mode.</li> </ul> <p>Returns:</p> <ul> <li><code>AnnData</code>: The loaded AnnData object containing gene expression data.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-_load_pytree","level":3,"title":"Function <code>_load_pytree</code>","text":"<p>Recursively load a PyTree from an HDF5 group.</p>","path":["API Reference"],"tags":[]},{"location":"api/#function-save_model","level":3,"title":"Function <code>save_model</code>","text":"<p>Save a Ginseng model state to a single HDF5 file.</p> <p>Parameters:</p> <ul> <li>state (<code>GinsengClassifierState</code>): Complete model state to save.</li> <li>path (<code>str | Path</code>): Path to the output HDF5 file (will add .h5 if not present).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # After training\n&gt;&gt;&gt; state = GinsengClassifierState(\n...     params=model.params,\n...     genes=dataset.gene_names,\n...     label_keys=dataset.label_names,\n...     label_values=np.arange(len(dataset.label_names)),\n...     n_genes=dataset.n_genes,\n...     n_classes=len(dataset.label_names),\n...     hidden_dim=256,\n...     normalize=True,\n...     target_sum=1e4,\n...     dropout_rate=0.5,\n...     training=False,\n... )\n&gt;&gt;&gt; save_model(state, \"./models/my_classifier.h5\")\n</code></pre>","path":["API Reference"],"tags":[]},{"location":"api/#function-_save_pytree","level":3,"title":"Function <code>_save_pytree</code>","text":"<p>Recursively save a PyTree to an HDF5 group.</p>","path":["API Reference"],"tags":[]},{"location":"api/#function-load_model","level":3,"title":"Function <code>load_model</code>","text":"<p>Load a Ginseng model state from an HDF5 file.</p> <p>Parameters:</p> <ul> <li>path (<code>str | Path</code>): Path to the HDF5 file.</li> </ul> <p>Returns:</p> <ul> <li><code>GinsengClassifierState</code>: Complete model state ready for inference or continued training.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; state = load_model(\"./models/my_classifier.h5\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Recreate model\n&gt;&gt;&gt; model = GinsengClassifier(\n...     n_genes=state.n_genes,\n...     n_classes=state.n_classes,\n...     hidden_dim=state.hidden_dim,\n...     dropout_rate=state.dropout_rate,\n...     normalize=state.normalize,\n...     target_sum=state.target_sum,\n... )\n&gt;&gt;&gt; model.params = state.params\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Prepare new data with correct gene order\n&gt;&gt;&gt; new_data_ordered = new_data[state.genes]\n&gt;&gt;&gt; predictions = model.predict(new_data_ordered.values, training=False)\n</code></pre>","path":["API Reference"],"tags":[]},{"location":"api/#file-modelnnpy","level":2,"title":"File: <code>model/nn.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#class-ginsengclassifier","level":3,"title":"Class <code>GinsengClassifier</code>","text":"<p>Wrapper class for cell type annotation with automatic key management.</p> <p>Attributes:</p> <ul> <li>n_genes (<code>int</code>): Number of genes in input data.</li> <li>n_classes (<code>int</code>): Number of cell type classes.</li> <li>hidden_dim (<code>int</code>): Hidden dimension for attention mechanism (default: 256).</li> <li>dropout_rate (<code>float</code>): Dropout rate during training (default: 0.5).</li> <li>normalize (<code>bool</code>): Whether to normalize input data (default: True).</li> <li>target_sum (<code>None | float</code>): Target sum for normalization (default: 1e4).</li> <li>seed (<code>int</code>): Random seed for reproducibility (default: 42).</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = GinsengClassifier(n_genes=2000, n_classes=10)\n&gt;&gt;&gt; # During training (JAX-compatible)\n&gt;&gt;&gt; loss = model.loss(model.params, model._get_key(), x_batch, y_batch)\n&gt;&gt;&gt; # For prediction (Standard usage)\n&gt;&gt;&gt; logits = model.predict(x_test, training=False)\n</code></pre>","path":["API Reference"],"tags":[]},{"location":"api/#method-_get_key","level":4,"title":"Method <code>_get_key</code>","text":"<p>Get a new PRNG key and update internal state.</p> <p>Returns:</p> <ul> <li><code>Array</code>: A new JAX PRNG key.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#method-predict","level":4,"title":"Method <code>predict</code>","text":"<p>Generate predictions for input data.</p> <p>Parameters:</p> <ul> <li>x (<code>Array</code>): Input gene expression matrix.</li> <li>params (<code>PyTree, optional</code>): Model parameters. If None, uses internal self.params.</li> <li>key (<code>Array, optional</code>): PRNG key. If None, uses a new key from self._get_key().</li> <li>training (<code>bool</code>): Whether to use training mode (with dropout).</li> <li>return_attn (<code>bool</code>): Whether to return attention weights.</li> </ul> <p>Returns:</p> <ul> <li><code>Array or tuple</code>: Logits, or (logits, attention) if return_attn=True.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#method-loss","level":4,"title":"Method <code>loss</code>","text":"<p>Compute cross-entropy loss for a batch.</p> <p>Parameters:</p> <ul> <li>params (<code>PyTree</code>): Model parameters to differentiate against.</li> <li>key (<code>Array</code>): PRNG key for dropout randomness.</li> <li>x (<code>Array</code>): Input gene expression matrix.</li> <li>y (<code>Array</code>): True class labels.</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Scalar loss value.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#method-evaluate","level":4,"title":"Method <code>evaluate</code>","text":"<p>Evaluate model on a batch and return loss and accuracy.</p> <p>Parameters:</p> <ul> <li>x (<code>Array</code>): Input gene expression matrix.</li> <li>y (<code>Array</code>): True class labels.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple[float, float]</code>: (loss, accuracy) on the batch.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-nn_xavier_uniform","level":3,"title":"Function <code>nn_xavier_uniform</code>","text":"<p>Initialize weights with Xavier uniform distribution.</p> <p>Parameters:</p> <ul> <li>key (<code>Array</code>): PRNG key array for random number generation.</li> <li>shape (<code>tuple of int</code>): Shape of the weight matrix.</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Initialized weight matrix.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-nn_init_linear","level":3,"title":"Function <code>nn_init_linear</code>","text":"<p>Initialize parameters for a linear layer.</p> <p>Parameters:</p> <ul> <li>key (<code>Array</code>): PRNG key array for random initialization.</li> <li>in_dim (<code>int</code>): Input dimension.</li> <li>out_dim (<code>int</code>): Output dimension.</li> </ul> <p>Returns:</p> <ul> <li><code>PyTree[Float[Array, \"...\"]]</code>: Dictionary with weight matrix <code>W</code> and bias vector <code>b</code>.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-nn_linear","level":3,"title":"Function <code>nn_linear</code>","text":"<p>Apply a linear transformation.</p> <p>Parameters:</p> <ul> <li>params (<code>PyTree[Float[Array, \"...\"]]</code>): Layer parameters with <code>W</code> and <code>b</code>.</li> <li>x (<code>Array</code>): Input tensor.</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Transformed output tensor.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-nn_dropout","level":3,"title":"Function <code>nn_dropout</code>","text":"<p>Apply dropout to input array with optimized implementation.</p> <p>Parameters:</p> <ul> <li>key (<code>Array</code>): PRNG key array for dropout mask.</li> <li>x (<code>Array</code>): Input tensor.</li> <li>rate (<code>float</code>): Dropout probability.</li> <li>training (<code>bool</code>): If True, apply dropout. Set to False for deterministic output after training.</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Tensor with dropout applied.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-nn_normalize","level":3,"title":"Function <code>nn_normalize</code>","text":"<p>Normalize counts per cell and apply log transform.</p> <p>Parameters:</p> <ul> <li>x (<code>Array</code>): Count matrix.</li> <li>target_sum (<code>float</code>): Target total count per cell after normalization.</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Normalized expression matrix.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-nn_annotate_init","level":3,"title":"Function <code>nn_annotate_init</code>","text":"<p>Initialize parameters for the cell type annotation network.</p> <p>Parameters:</p> <ul> <li>key (<code>Array</code>): PRNG key array for random initialization.</li> <li>n_genes (<code>int</code>): Number of genes.</li> <li>n_classes (<code>int</code>): Number of classes.</li> <li>hidden_dim (<code>int</code>): Dimension of hidden layers for attention mechanism.</li> </ul> <p>Returns:</p> <ul> <li><code>PyTree[Float[Array, \"...\"]]</code>: Dictionary of initialized model parameters.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-nn_annotate","level":3,"title":"Function <code>nn_annotate</code>","text":"<p>Annotate cells using instance-based attention neural network.</p> <p>Parameters:</p> <ul> <li>params (<code>PyTree[Float[Array, \"...\"]]</code>): Model parameters from <code>nn_annotate_init</code>.</li> <li>key (<code>Array</code>): PRNG key array for random number generation.</li> <li>x (<code>Array</code>): Input gene expression matrix (batch_size, n_genes).</li> <li>dropout_rate (<code>float</code>): Dropout probability (default: 0.5).</li> <li>normalize (<code>bool</code>): If True, normalize and log-transform counts (default: True).</li> <li>target_sum (<code>None | float</code>): Target total count per cell after normalization (default: 1e4).</li> <li>return_attn (<code>bool</code>): If True, also return attention weights (default: False).</li> <li>training (<code>bool</code>): Whether the model is in training mode (default: True).</li> </ul> <p>Returns:</p> <ul> <li><code>Float[Array, ...] | tuple[Float[Array, ...], Float[Array, ...]]</code>: Logits for each class. If <code>return_attn=True</code>, gene-level attention weights are also returned as second element.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-nn_annotate_loss","level":3,"title":"Function <code>nn_annotate_loss</code>","text":"<p>Cross-entropy loss for cell type annotation model.</p> <p>Parameters:</p> <ul> <li>params (<code>PyTree[Float[Array, \"...\"]]</code>): Model parameters from <code>nn_annotate_init</code>.</li> <li>key (<code>Array</code>): PRNG key array for random number generation.</li> <li>x (<code>Array</code>): Input gene expression matrix (batch_size, n_genes).</li> <li>y (<code>Array</code>): True cell type labels as integer class indices (batch_size,).</li> <li>dropout_rate (<code>float</code>): Dropout probability (default: 0.5).</li> <li>normalize (<code>bool</code>): If True, normalize and log-transform counts (default: True).</li> <li>target_sum (<code>None | float</code>): Target total count per cell after normalization. Defaults to 1e4 (standard for scRNA-seq).</li> </ul> <p>Returns:</p> <ul> <li><code>Float[Array, \"\"]</code>: Scalar cross-entropy loss.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-nn_annotate_evaluate","level":3,"title":"Function <code>nn_annotate_evaluate</code>","text":"<p>Evaluate model performance on a batch.</p> <p>Parameters:</p> <ul> <li>params (<code>PyTree[Float[Array, \"...\"]]</code>): Model parameters from <code>nn_annotate_init</code>.</li> <li>key (<code>Array</code>): PRNG key array for random number generation.</li> <li>x (<code>Array</code>): Input gene expression matrix (batch_size, n_genes).</li> <li>y (<code>Array</code>): True cell type labels as integer class indices (batch_size,).</li> <li>dropout_rate (<code>float</code>): Dropout probability (default: 0.5).</li> <li>normalize (<code>bool</code>): If True, normalize and log-transform counts (default: True).</li> <li>target_sum (<code>None | float</code>): Target total count per cell after normalization. Defaults to 1e4 (standard for scRNA-seq).</li> </ul> <p>Returns:</p> <ul> <li><code>tuple[Float[Array, \"\"], Int[Array, \"\"], Int[Array, \"\"]]</code>: Tuple of (loss, total_samples, correct_predictions).</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#file-modelpredictpy","level":2,"title":"File: <code>model/predict.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#function-_get_gene_indices","level":3,"title":"Function <code>_get_gene_indices</code>","text":"<p>Map model gene names to indices within the AnnData object.</p> <p>Parameters:</p> <ul> <li>adata (<code>AnnData</code>): The single-cell data object.</li> <li>gene_names (<code>np.ndarray</code>): Array of gene names expected by the model in a specific order.</li> <li>gene_key (<code>str | None</code>): Column name in <code>adata.var</code> containing gene names. If None, uses <code>adata.var_names</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>available_idx</code>: np.ndarray The integer indices of genes found in <code>adata</code> that match the model's expected genes. out_positions : np.ndarray The corresponding positions in the model's input vector where these genes belong.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-annotate_iter","level":3,"title":"Function <code>annotate_iter</code>","text":"<p>Batch iterator that handles gene reordering and zero-padding for missing genes.</p> <p>Parameters:</p> <ul> <li>adata (<code>AnnData</code>): The single-cell data object.</li> <li>gene_names (<code>np.ndarray</code>): Ordered gene names from the trained model.</li> <li>gene_key (<code>str | None</code>): Column in <code>adata.var</code> to use for gene matching.</li> <li>layer (<code>str | None</code>): Key in <code>adata.layers</code> to use for counts. If None, uses <code>adata.X</code>.</li> <li>batch_size (<code>int</code>): Number of cells to process per batch.  Yields ------</li> <li>batch_tensor (<code>jax.numpy.ndarray</code>): A JAX-compatible array of shape (batch_size, n_model_genes).</li> <li>start (<code>int</code>): Starting observation index.</li> <li>end (<code>int</code>): Ending observation index.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-classify","level":3,"title":"Function <code>classify</code>","text":"<p>Annotate single-cell sequencing data using a trained ginseng classifier.</p> <p>Parameters:</p> <ul> <li>model_state (<code>GinsengClassifierState | str | Path</code>): A loaded GinsengClassifierState or a path to a saved state file.</li> <li>adata (<code>AnnData | str | Path</code>): AnnData object or path to count data (.h5ad, .h5, or Matrix Market).</li> <li>gene_key (<code>str | None</code>): Column in <code>.var</code> containing gene names. If None, uses index.</li> <li>layer (<code>str | None</code>): Key in <code>adata.layers</code> to use for counts. If None, uses <code>adata.X</code>.</li> <li>backed (<code>bool</code>): If True and <code>adata</code> is a path, reads data in backed mode.</li> <li>normalize (<code>bool | None</code>): Override model normalization setting.</li> <li>target_sum (<code>float | None</code>): Override model target sum.</li> <li>randomness (<code>bool</code>): If True, enables dropout during inference.</li> <li>batch_size (<code>int</code>): Number of cells to process in each forward pass.</li> <li>copy (<code>bool</code>): If True, returns a modified copy of the AnnData object.</li> <li>store_probs (<code>bool</code>): If True, stores the full probability matrix in <code>adata.obsm</code>.</li> <li>return_table (<code>bool</code>): If True, returns a pandas DataFrame instead of modifying AnnData.</li> <li>seed (<code>int</code>): Random seed.</li> <li>silent (<code>bool</code>): If True, suppresses progress bar and warnings.</li> </ul> <p>Returns:</p> <ul> <li><code>AnnData | pd.DataFrame | None</code>: Returns a DataFrame if <code>return_table</code> is True, a copy of AnnData if <code>copy</code> is True, otherwise modifies in-place and returns None.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#file-modelstatepy","level":2,"title":"File: <code>model/state.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#class-ginsengclassifierstate","level":3,"title":"Class <code>GinsengClassifierState</code>","text":"<p>Complete state of a trained Ginseng model.</p> <p>All information needed to save, load, and use a trained model.</p> <p>Attributes:</p> <ul> <li>params (<code>PyTree</code>): Model parameters (weights and biases).</li> <li>genes (<code>np.ndarray</code>): Gene names in the exact order expected by the model.</li> <li>label_keys (<code>np.ndarray</code>): Label names (e.g., ['T-cell', 'B-cell', 'Macrophage']).</li> <li>label_values (<code>np.ndarray</code>): Integer values corresponding to each label (e.g., [0, 1, 2]).</li> <li>n_genes (<code>int</code>): Number of genes.</li> <li>n_classes (<code>int</code>): Number of classes.</li> <li>hidden_dim (<code>int</code>): Hidden dimension used in attention mechanism.</li> <li>normalize (<code>bool</code>): Whether input data should be normalized.</li> <li>target_sum (<code>float</code>): Target sum for normalization.</li> <li>dropout_rate (<code>float</code>): Dropout rate used during training.</li> <li>training (<code>bool</code>): Whether weights should be frozen (False after training).</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-classifier_from_state","level":3,"title":"Function <code>classifier_from_state</code>","text":"<p>Create a GinsengClassifier from a loaded state.</p> <p>Parameters:</p> <ul> <li>state (<code>GinsengClassifierState</code>): Loaded model state.</li> </ul> <p>Returns:</p> <ul> <li><code>GinsengClassifier</code>: Model ready for inference.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-state_from_classifier_trainer","level":3,"title":"Function <code>state_from_classifier_trainer</code>","text":"<p>Create a GinsengClassifierState from a trainer after .fit().</p> <p>Parameters:</p> <ul> <li>trainer (<code>GinsengClassifierTrainer</code>): Trainer instance with a trained model.</li> </ul> <p>Returns:</p> <ul> <li><code>GinsengClassifierState</code>: Complete model state.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trainer = GinsengClassifierTrainer(dataset, settings)\n&gt;&gt;&gt; trainer.fit(epochs=50)\n&gt;&gt;&gt; state = state_from_trainer(trainer)\n&gt;&gt;&gt; save_model(state, \"./models/my_classifier.h5\")\n</code></pre>","path":["API Reference"],"tags":[]},{"location":"api/#file-trainaugmentpy","level":2,"title":"File: <code>train/augment.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#function-augment_mask","level":3,"title":"Function <code>augment_mask</code>","text":"<p>Randomly mask out counts across cells.</p> <p>Parameters:</p> <ul> <li>key (<code>Array</code>): PRNG key array for dropout mask.</li> <li>x (<code>Array</code>): Input tensor.</li> <li>rate (<code>float</code>): Dropout probability.</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Tensor with dropout applied.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-augment_background","level":3,"title":"Function <code>augment_background</code>","text":"<p>Randomly add Poisson-distributed background noise to counts.</p> <p>Parameters:</p> <ul> <li>key (<code>Array</code>): PRNG key array for noise generation.</li> <li>x (<code>Array</code>): Non-normalized count matrix.</li> <li>lam_max (<code>float</code>): Maximum mean of Poisson distribution for sampling added noise.</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Counts with added Poisson noise.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-augment_dropgene","level":3,"title":"Function <code>augment_dropgene</code>","text":"<p>Randomly zero out entire genes.</p> <p>Parameters:</p> <ul> <li>key (<code>Array</code>): PRNG key array for mask generation.</li> <li>x (<code>Array</code>): Count matrix.</li> <li>lower (<code>int</code>): Minimum number of genes to mask out.</li> <li>upper (<code>int</code>): Maximum (exclusive) number of genes to mask out.</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Filtered counts.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-augment","level":3,"title":"Function <code>augment</code>","text":"<p>Apply a combination of single-cell RNA relevant augmentations.</p> <p>Parameters:</p> <ul> <li>key (<code>Array</code>): PRNG key array for mask generation.</li> <li>x (<code>Array</code>): Count matrix.</li> <li>rate (<code>float</code>): Dropout probability.</li> <li>lam_max (<code>float</code>): Maximum mean of Poisson distribution for sampling added noise.</li> <li>lower (<code>int</code>): Minimum number of genes to mask out.</li> <li>upper (<code>int</code>): Maximum (exclusive) number of genes to mask out.</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Augmented counts.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#file-trainloggerpy","level":2,"title":"File: <code>train/logger.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#class-ginsenglogger","level":3,"title":"Class <code>GinsengLogger</code>","text":"<p>Logger for storing training and validation metrics across epochs.</p> <p>Attributes:</p> <ul> <li>epoch (<code>list[int]</code>): List of epoch indices.</li> <li>train_loss (<code>list[float]</code>): Training loss values for each epoch.</li> <li>holdout_loss (<code>list[float]</code>): Holdout loss values for each epoch.</li> <li>holdout_accuracy (<code>list[float]</code>): Holdout accuracy values for each epoch.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#method-update","level":4,"title":"Method <code>update</code>","text":"<p>Update the logger with new training and validation metrics.</p> <p>Parameters:</p> <ul> <li>epoch (<code>int</code>): Current epoch index.</li> <li>train_loss (<code>float</code>): Training loss at this epoch.</li> <li>holdout_loss (<code>float</code>): Validation loss at this epoch.</li> <li>holdout_accuracy (<code>float</code>): Validation accuracy at this epoch.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#method-report","level":4,"title":"Method <code>report</code>","text":"<p>Print most recent result to standard output.</p> <p>Parameters:</p> <ul> <li>silent (<code>bool</code>): If True, suppresses report output.</li> <li>flush (<code>bool</code>): If True, write report to output immediately.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code>: Output is printed to standard output.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#file-trainoptpy","level":2,"title":"File: <code>train/opt.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#class-adamstate","level":3,"title":"Class <code>AdamState</code>","text":"<p>State for the Adam optimizer.</p> <p>Attributes:</p> <ul> <li>step (<code>int</code>): Current optimization step.</li> <li>m (<code>PyTree of Array</code>): Exponential moving average of gradients.</li> <li>v (<code>PyTree of Array</code>): Exponential moving average of squared gradients.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-opt_init_adam","level":3,"title":"Function <code>opt_init_adam</code>","text":"<p>Initialize Adam optimizer state.</p> <p>Parameters:</p> <ul> <li>params (<code>PyTree[Float[Array, ...]]</code>): Model parameters to be optimized.</li> </ul> <p>Returns:</p> <ul> <li><code>AdamState</code>: Initial optimizer state with zeroed moments.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#function-opt_adam_update","level":3,"title":"Function <code>opt_adam_update</code>","text":"<p>Perform one Adam optimization step.</p> <p>Parameters:</p> <ul> <li>grads (<code>PyTree[Float[Array, ...]]</code>): Gradients of the loss w.r.t. parameters.</li> <li>params (<code>PyTree[Float[Array, ...]]</code>): Current model parameters.</li> <li>state (<code>AdamState</code>): Current optimizer state.</li> <li>lr (<code>float</code>): Learning rate.</li> <li>betas (<code>tuple[float, float]</code>): Exponential decay rates for first and second moment estimates.</li> <li>eps (<code>float</code>): Numerical stability constant.</li> <li>weight_decay (<code>float</code>): L2 regularization factor.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: Updated parameters and new optimizer state.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#file-traintrainerpy","level":2,"title":"File: <code>train/trainer.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#class-ginsengclassifiertrainersettings","level":3,"title":"Class <code>GinsengClassifierTrainerSettings</code>","text":"<p>Training configuration settings for <code>GinsengClassifierTrainer</code>.</p>","path":["API Reference"],"tags":[]},{"location":"api/#class-ginsengclassifiertrainer","level":3,"title":"Class <code>GinsengClassifierTrainer</code>","text":"<p>Trainer class for orchestrating the training of a GinsengClassifier.</p>","path":["API Reference"],"tags":[]},{"location":"api/#method-_train_step","level":4,"title":"Method <code>_train_step</code>","text":"<p>Internal training step to update parameters.</p>","path":["API Reference"],"tags":[]},{"location":"api/#method-fit","level":4,"title":"Method <code>fit</code>","text":"<p>Execute the training loop.</p> <p>Parameters:</p> <ul> <li>epochs (<code>int</code>): Number of training epochs to run (default : 10).</li> <li>silent (<code>bool</code>): If True, suppresses training progress output (default : False).</li> </ul> <p>Returns:</p> <ul> <li><code>GinsengClassifier</code>: The trained classifier with updated parameters.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"api/#method-_run_epoch","level":4,"title":"Method <code>_run_epoch</code>","text":"<p>Run a single training epoch.</p>","path":["API Reference"],"tags":[]},{"location":"api/#method-_validate","level":4,"title":"Method <code>_validate</code>","text":"<p>Run validation on the holdout split.</p>","path":["API Reference"],"tags":[]},{"location":"api/#file-utilshvgpy","level":2,"title":"File: <code>utils/hvg.py</code>","text":"","path":["API Reference"],"tags":[]},{"location":"api/#function-select_hvgs","level":3,"title":"Function <code>select_hvgs</code>","text":"<p>Select highly variable genes from raw or normalized counts.</p> <p>Parameters:</p> <ul> <li>adata (<code>AnnData</code>): AnnData object with gene names stored in <code>var</code>.</li> <li>n_top_genes (<code>int</code>): Number of top highly variable genes to select.</li> <li>layer (<code>str, optional</code>): Key in <code>adata.layers</code> to use. If None, uses <code>adata.X</code>.</li> <li>target_sum (<code>float, optional</code>): If provided, scales each cell to this sum and applies log1p transformation (default: 1e4).</li> <li>min_mean (<code>float</code>): Lower quantile bound on mean gene expression.</li> <li>max_mean (<code>float</code>): Upper quantile bound on mean gene expression.</li> <li>n_bins (<code>int</code>): Select genes across this many gene expression bins.</li> <li>chunk_size (<code>int</code>): Number of cells to process in memory at once.</li> <li>silent (<code>bool</code>): If True, suppresses progress bar.</li> <li>copy (<code>bool</code>): If True, returns a copy of the AnnData.</li> </ul> <p>Returns:</p> <ul> <li><code>Optional[AnnData]</code>: Marks highly variable genes in <code>var['ginseng_genes']</code>.</li> </ul>","path":["API Reference"],"tags":[]},{"location":"guides/basic/","level":1,"title":"Usage","text":"","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#ginsengdataset","level":2,"title":"<code>GinsengDataset</code>","text":"<p><code>GinsengDataset</code> is the core object for training single-cell annotation models in <code>ginseng</code>. It is designed to convert large single-cell datasets (from <code>.h5ad</code>, 10x, or URLs) into a compressed on-disk <code>zarr</code> store for training. This allows for memory-efficient training by streaming mini-batches without loading the entire dataset into RAM.</p>","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#creating-a-ginsengdataset","level":3,"title":"Creating a <code>GinsengDataset</code>","text":"<p>To convert your raw counts into a <code>GinsengDataset</code>, the <code>create</code> class method can be used. The <code>.create</code> method handles gene subsetting, label encoding, and disk serialization.</p> <pre><code>from ginseng.data import GinsengDataset\n\n# Create a dataset from an existing AnnData object or path\nds = GinsengDataset.create(\n    path=\"my_dataset.zarr\",          # Output directory\n    adata=\"path/to/counts.h5ad\",     # Input data (Path, URL, or AnnData)\n    label_key=\"cell_type\",           # Target labels in adata.obs\n    layer=\"counts\",                  # Optional: specify a layer (e.g., raw counts)\n    genes=\"highly_variable\",         # Optional: use a mask in adata.var\n    group_key=\"batch\"                # Optional: metadata for stratified splitting\n)\n</code></pre>","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#loading-an-existing-ginsengdataset","level":3,"title":"Loading an existing <code>GinsengDataset</code>","text":"<p>If you have already created a <code>GinsengDataset</code> and saved it to disk, you can load it directly by providing the path to the <code>zarr</code> store.</p> <pre><code># Load an existing dataset from disk\nds = GinsengDataset(\"dataset.zarr\")\n</code></pre>","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#initializing-traintest-splits","level":3,"title":"Initializing train/test splits","text":"<p>Once the Zarr store is created, you can define your data splits. Ginseng stores these indices inside the Zarr group so they persist across sessions.</p> <pre><code># Create a 90/10 train/test split\nds.make_split(fraction=0.1, stratify_group=True)\n</code></pre> <p>If you used <code>group_key</code> when creating the <code>GinsengDataset</code>, then setting <code>stratify_group=True</code> will ensure that all cells from the same group instance (e.g. batch, donor, etc.) are kept together in either the training or test set.</p>","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#streaming-mini-batches","level":3,"title":"Streaming mini-batches","text":"<p>The stream method provides a python iterator that yields jax/numpy compatible arrays. This is where the on-disk performance provides major benefits as only the required chunks are decompressed during iteration.</p> <pre><code># Stream mini-batches for training\nfor X_batch, y_batch in ds.stream(batch_size=256, split=\"train\", shuffle=True, balance_labels=True):\n    # X_batch shape: (256, n_genes)\n    # y_batch shape: (256,)\n    # Perform training step here...\n    pass\n</code></pre> <p>The <code>stream</code> method supports shuffling, different splits (<code>train</code>, <code>test</code>, or <code>all</code>), adjustable batch sizes, and can also enforce balanced sampling of labels during training.</p>","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#ginsengclassifier","level":2,"title":"<code>GinsengClassifier</code>","text":"<p>Training cell type annotation models in <code>ginseng</code> is managed by the <code>GinsengClassifierTrainer</code>. This class handles the <code>jax</code>-based optimization loop, handles on-the-fly data augmentation, and manages model evaluation on holdout splits.</p>","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#setting-training-parameters","level":3,"title":"Setting training parameters","text":"<p>A <code>GinsengClassifier</code> can be trained using a variety of model, optimization, and augmentation hyperparameters. These can be set using the <code>GinsengClassifierTrainerSettings</code> class.</p> <pre><code>from ginseng.train import GinsengClassifierTrainerSettings\n\nsettings = GinsengClassifierTrainerSettings(\n    # Augmentation parameters\n    rate=0.1,\n    lam_max=None,\n    lower=0,\n    upper=200,\n\n    # Model parameters\n    hidden_dim=64,\n    dropout_rate=0.2,\n    batch_size=128,\n\n    # Training parameters\n    lr=0.001,\n    betas=(0.9, 0.999),\n    eps=1e-8,\n    weight_decay=0.01,\n    normalize=True,\n    target_sum=1e4,\n    holdout_fraction=0.05,\n    balance_train=True,\n    group_level=False,\n    group_mode=\"fraction\",\n\n    # Random seed for reproducibility\n    seed=123\n)\n</code></pre>","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#training-a-model","level":3,"title":"Training a model","text":"<p>Once you've specified your dataset and training parameters, you can initialize a <code>GinsengClassifierTrainer</code> to begin training.</p> <pre><code>from ginseng.train import GinsengClassifierTrainer\n\n# Initialize trainer\ntrainer = GinsengClassifierTrainer(dataset, settings)\n\n# Fit the model\nmodel, state = trainer.fit(epochs=10)\n</code></pre> <p>After training, the <code>fit</code> method returns the trained model and its state, which can be used for inference or saved to disk.</p>","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#saving-and-loading-model-state","level":3,"title":"Saving and loading model state","text":"<p>The model state can be saved to disk for later use or inference.</p> <pre><code>from ginseng.data.io import save_model, load_model\n\n# Save model state\nsave_model(state, \"model.h5\")\n\n# Load model state\nloaded_state = load_model(\"model.h5\")\n</code></pre>","path":["Guides","Usage"],"tags":[]},{"location":"guides/basic/#annotating-new-data","level":2,"title":"Annotating new data","text":"<p>Once you have trained a model, you can either directly use the model on new data by iterating over the dataset. However, the recommended approach is to use the <code>ginseng.classify</code> function, which uses the model state to correctly subset, order, and account for missing genes. Furthermore, it's optimized to work with backed AnnData objects which makes it easy to annotate large datasets without having to load everything into memory.</p> <pre><code>import ginseng\nfrom ginseng.data.io import read_adata, load_model\n\n# Load model\nstate = load_model(\"model.h5\")\n\n# Load AnnData\nadata = read_adata(\"data.h5ad\")\n\n# Classify new data in-place\nginseng.classify(data, state, layer=\"counts\")\nassert \"ginseng_cell_type\" in adata.obs\nassert \"ginseng_confidence\" in adata.obs\n\n# Return predictions as a separate table\npredictions = ginseng.classify(data, state, layer=\"counts\", return_table=True)\n</code></pre>","path":["Guides","Usage"],"tags":[]},{"location":"guides/uterine/","level":1,"title":"Training a model to annotate new data","text":"<p>In this tutorial, we provide a step-by-step guide on how to train a <code>ginseng</code> model, and how to use it to annotate new data. We will use single-cell RNA-seq data from human uterine tissue for this example.</p>","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#load-data","level":2,"title":"Load data","text":"<p>The <code>read_adata</code> function from <code>ginseng.data.io</code> can be used to load 10x matrix market data, 10x <code>h5</code> data, or in-memory or backed <code>AnnData</code> objects. In addition, as we show here, data can be downloaded and loaded directly by providing a URL endpoint.</p> <pre><code>from ginseng.data.io import read_adata\n\n# Load uterine data from Ulrich et al. 2024\ntrain = read_adata(\"https://datasets.cellxgene.cziscience.com/273ba93d-0751-4035-b1e1-d5c3a614beae.h5ad\")\n\n# Load uterine data from Tabula Sapiens\ntest = read_adata(\"https://datasets.cellxgene.cziscience.com/42f6f928-f6ef-41f5-9fed-4054027552d7.h5ad\")\n</code></pre>","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#preprocess-data","level":2,"title":"Preprocess data","text":"","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#subset-cell-types","level":3,"title":"Subset cell types","text":"<p>For simplicity, we will only train on cell types with more than 10 cells in both the training and test datasets. As <code>ginseng</code> takes raw counts as input, no preprocessing other than defining the subset of genes you wish to train on is necessary. For ease-of-use, <code>ginseng</code> provides a <code>select_hvgs</code> function that works on both in-memory and backed <code>AnnData</code> objects. If a backed object is provided, the HVGs are computed using a chunk-based strategy to avoid loading the entire dataset into memory.</p> <pre><code># Count the number of cells per cell type in train and test\ntrain_counts = train.obs.cell_type.value_counts().reset_index()\ntest_counts = test.obs.cell_type.value_counts().reset_index()\n\n# Merge counts\nmerged_counts = train_counts.merge(\n    test_counts,\n    on=\"cell_type\",\n    how=\"outer\",\n    suffixes=(\"_train\", \"_test\")\n).fillna(0)\n\n# Retain only cell types present in both datasets\nmerged_counts = merged_counts.loc[(merged_counts[\"count_train\"] &gt; 10) &amp; (merged_counts[\"count_test\"] &gt; 10)]\n\n# Subset train and test to only these cell types\ntrain = train[train.obs.cell_type.isin(merged_counts[\"cell_type\"])].copy()\ntest = test[test.obs.cell_type.isin(merged_counts[\"cell_type\"])].copy()\n\n# Store the raw counts\ntrain.layers[\"counts\"] = train.raw.X.copy()\ntest.layers[\"counts\"] = test.raw.X.copy()\n</code></pre> <p>The final set of cell types we will be annotating in this tutorial.</p> cell_type count_train count_test B cell 140 50 ciliated epithelial cell 4475 320 fibroblast 1585 6593 macrophage 3158 929 mast cell 882 65 natural killer cell 5356 92","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#select-highly-variable-genes","level":3,"title":"Select highly variable genes","text":"<p>Now we can select the highly variable genes (HVGs) from the training dataset.</p> <pre><code>from ginseng.utils import select_hvgs\n\n# Select highly variable genes (stored in train.var['ginseng_genes'])\nselect_hvgs(train, n_top_genes=2500, layer=\"counts\")\n</code></pre> <pre><code>[ginseng] Selecting genes: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02&lt;00:00,  1.76 chunks/s]\n</code></pre>","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#construct-a-ginsengdataset","level":3,"title":"Construct a <code>GinsengDataset</code>","text":"<p>We can now setup a <code>GinsengDataset</code> which enables efficient data loading during training.</p> <pre><code>from ginseng.data import GinsengDataset\n\ndataset = GinsengDataset.create(\"train.zarr\", train, layer=\"counts\", genes=\"ginseng_genes\", label_key=\"cell_type\")\n</code></pre> <pre><code>[ginseng] Writing zarr: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00,  8.37it/s]\n</code></pre> <p>If you didn't want to train the model now, you can retain the <code>GinsengDataset</code> on disk for later use, and re-load it as follows.</p> <pre><code>dataset = GinsengDataset(\"train.zarr\")\n</code></pre>","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#train-a-classifier","level":2,"title":"Train a classifier","text":"<p>For classification, all the required machinery for training a model is encapsulated in the <code>GinsengClassifierTrainer</code> class. This class takes care of setting up the model, optimizer, and data loaders, as well as the training loop itself. Below, we initialize a trainer using the dataset we created above.</p> <pre><code>from ginseng.train import GinsengClassifierTrainer, GinsengClassifierTrainerSettings\n\nsettings = GinsengClassifierTrainerSettings(\n    # Augmentation parameters\n    rate=0.1,\n    lam_max=None,\n    lower=0,\n    upper=200,\n\n    # Model parameters\n    hidden_dim=64,\n    dropout_rate=0.2,\n    batch_size=128,\n\n    # Training parameters\n    lr=0.001,\n    betas=(0.9, 0.999),\n    eps=1e-8,\n    weight_decay=0.01,\n    normalize=True,\n    target_sum=1e4,\n    holdout_fraction=0.05,\n    balance_train=True,\n    group_level=False,\n    group_mode=\"fraction\",\n\n    # Random seed for reproducibility\n    seed=123\n)\n\ntrainer = GinsengClassifierTrainer(dataset, settings)\n</code></pre> <p>Now we can train our model by calling the <code>fit</code> method on the trainer. The trained model and model state are returned after training is complete.</p> <p><pre><code>model, state = trainer.fit(epochs=6, silent=False)\n</code></pre> <pre><code>[ginseng] Epoch 1 report |  Training loss: 1.535e+00 | Holdout loss: 6.632e-01 | Holdout accuracy: 8.557e-01 |\n[ginseng] Epoch 2 report |  Training loss: 4.782e-01 | Holdout loss: 3.550e-01 | Holdout accuracy: 9.483e-01 |\n[ginseng] Epoch 3 report |  Training loss: 1.813e-01 | Holdout loss: 3.831e-01 | Holdout accuracy: 9.546e-01 |\n[ginseng] Epoch 4 report |  Training loss: 1.747e-01 | Holdout loss: 2.428e-01 | Holdout accuracy: 9.572e-01 |\n[ginseng] Epoch 5 report |  Training loss: 1.144e-01 | Holdout loss: 2.446e-01 | Holdout accuracy: 9.539e-01 |\n[ginseng] Epoch 6 report |  Training loss: 5.886e-02 | Holdout loss: 2.288e-01 | Holdout accuracy: 9.516e-01 |\n</code></pre></p>","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#annotate-new-data","level":2,"title":"Annotate new data","text":"<p>If you are familiar with neural networks and <code>jax</code>, the model can be used to construct custom inference or training loops. However, for convenience, <code>ginseng</code> provides a simple API for annotating new datasets that only requires the model state. Assuming the same gene identifiers are present in the new dataset, annotation can be performed as follows.</p>","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#classify-cells","level":3,"title":"Classify cells","text":"<pre><code>import ginseng\n\nginseng.classify(state, test, layer=\"counts\")\n</code></pre> <pre><code>UserWarning: Partial gene overlap detected: 99.92%.\n\n[ginseng] Classifying: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00&lt;00:00, 48.37it/s]\n</code></pre> <p>The predicted cell type labels from <code>ginseng.classify</code> can be found in the <code>ginseng_cell_type</code> column of the AnnData <code>obs</code> dataframe. Additionaly, the maximum predicted probability for each cell is stored in the <code>ginseng_confidence</code> column, which can be used to filter low-confidence predictions.</p> <p>Note</p> <p>Any time there isn't a perfect overlap between the gene sets in the training and test sets, <code>ginseng</code> will provide a warning specifying the fraction of overlapping genes. However, <code>ginseng</code> will automatically handle missing genes by inserting zero-valued columns for those genes during inference. To train a model robust to missing genes, it is recommended to use dropout on the input layer during training (<code>rate</code> &gt; 0) and allow complete masking of genes during training (from <code>lower</code> to <code>upper</code>).</p>","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#evaluate-performance","level":3,"title":"Evaluate performance","text":"<p>We will make a confusion matrix to visualize the performance of our classifier on the test dataset.</p> <pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Construct a confusion matrix\nannotations = test.obs[[\"ginseng_cell_type\", \"cell_type\"]].copy()\nannotations[\"ginseng_cell_type\"] = annotations[\"ginseng_cell_type\"].astype(str)\nannotations[\"cell_type\"] = annotations[\"cell_type\"].astype(str)\nannotations['correct'] = annotations['ginseng_cell_type'] == annotations['cell_type']\n\nconfusion_matrix = pd.crosstab(\n    annotations['cell_type'],\n    annotations['ginseng_cell_type'],\n    rownames=['True Label'],\n    colnames=['Predicted Label'],\n    normalize='index'\n)\n\ncell_types = merged_counts['cell_type'].tolist()\nconfusion_matrix = confusion_matrix.reindex(index=cell_types, columns=cell_types, fill_value=0)\n\n\nfig, ax = plt.subplots(figsize=(5, 4))\nsns.heatmap(confusion_matrix, annot=True, fmt=\".2f\", cmap=\"cubehelix_r\", ax=ax)\ncbar = ax.collections[0].colorbar\ncbar.set_label('Accuracy', rotation=270, labelpad=15)\nax.set_title(\"Confusion matrix\")\nplt.show()\n</code></pre> <p></p> <p>We will also visualize the true labels and predicted labels on an embedding for qualitative inspection.</p> <pre><code>import numpy as np\nfrom sklearn.manifold import TSNE\n\n# Normalize counts\nX = test[:, test.var.index.isin(state.genes)].layers[\"counts\"].toarray()\nX = (1e4 * X.T / X.sum(axis=1)).T\nX = np.log1p(X)\n\n# Embedding\nz = TSNE(n_components=2, random_state=123, perplexity=60.0).fit_transform(X)\n\n# Plot annotated embeddings\nfig, ax = plt.subplots(1, 3, figsize=(14, 3))\n\ncell_type_palette = {k: v for k, v in zip(merged_counts['cell_type'], sns.color_palette('tab10', n_colors=len(merged_counts)))}\n\nsns.scatterplot(\n    x=z[:, 0], y=z[:, 1], hue=test.obs['cell_type'],\n    palette=cell_type_palette, s=5, alpha=0.8, ax=ax[0]\n)\n\nsns.scatterplot(\n    x=z[:, 0], y=z[:, 1], hue=test.obs['ginseng_cell_type'],\n    palette=cell_type_palette, s=5, alpha=0.8, ax=ax[1]\n)\n\nsns.scatterplot(\n    x=z[:, 0], y=z[:, 1], hue=test.obs['ginseng_confidence'],\n    palette='viridis', s=5, alpha=0.8, ax=ax[2],\n    hue_norm=(test.obs['ginseng_confidence'].min(), 1)\n)\n\nsns.move_legend(ax[0], \"upper left\", title=\"True cell type\", frameon=False, bbox_to_anchor=(1.05, 1), markerscale=3)\nsns.move_legend(ax[1], \"upper left\", title=\"ginseng cell type\", frameon=False, bbox_to_anchor=(1.05, 1), markerscale=3)\nsns.move_legend(ax[2], \"upper left\", title=\"ginseng confidence\", frameon=False, bbox_to_anchor=(1.05, 1), markerscale=3)\n\nfor i in range(3):\n    ax[i].axis('off')\n\nfig.tight_layout()\n</code></pre> <p></p>","path":["Guides","Training a model to annotate new data"],"tags":[]},{"location":"guides/uterine/#saving-and-loading-models","level":2,"title":"Saving and loading models","text":"<p><code>ginseng</code> models can be saved and loaded in a portable hdf5 format using the <code>save_model</code> and <code>load_model</code> functions. This allows you to save trained models to disk and load them later for inference or further training. For ease-of-use, the <code>ginseng.classify</code> function can perform classification directly from file paths pointing to the saved models and AnnData objects.</p> <pre><code>from ginseng.data.io import save_model, load_model\n\n# Save the trained model\nsave_model(state, \"toy_model.h5\")\n\n# Load the trained model\nloaded_state = load_model(\"toy_model.h5\")\n\n# Perform classification directly from saved model and AnnData path\nginseng.classify(\"toy_model.h5\", test, layer=\"counts\")\n</code></pre> <pre><code>UserWarning: Partial gene overlap detected: 99.92%.\n\n[ginseng] Classifying: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00&lt;00:00, 67.61it/s]\n</code></pre>","path":["Guides","Training a model to annotate new data"],"tags":[]}]}